\documentclass{report}
\usepackage{amsmath}
\usepackage{tabularx}


\begin{document}
\title{Clay-Wolkin Fellowship ISP: Mid-Year Report}
\author{Nick Draper, Christine Goins, Kaveh Pezeshki, Zunyan Wang, Nancy Wei}
\date{November 2017}
\maketitle

\tableofcontents
\listoffigures
\listoftables

\begin{abstract}
	\label{abstract}
\end{abstract}

\chapter{Introduction}

\chapter{Project Goals}
	\section{Constraints}
	\section{Hardware Specification}

\chapter{Background Information}
	\section{Image Signal Processing}
		\subsection{Typical ISP Pipelines}
	\section{Image Recognition via CNN}
	
\chapter{Microarchitecture Specifications}

\chapter{ISP Pipeline Testing}
	\section{Testing Goals}
	In order to establish a pipeline specification for the ISP, it was first necessary to discover what pipeline steps were necessary to achieve adequate machine learning performance, and furthermore, what algorithms were readily available, and easy to implement in hardware, to accomplish these steps.
	\section{Background}
		\subsection{RAW Conversion}
		In industry, tools such as Adobe Photoshop and Lightroom are often used to edit and convert RAW image formats to standardized formats such as PNG and JPG\footnote{https://digital-photography-school.com/raw-workflow-a-pros-approach/}. However, these products are 'black boxes' and therefore inadequate for the Fellowship's approach to optimizing an ISP for machine learning.
		
		The team therefore investigated the open-source tools UFraw\footnote{http://ufraw.sourceforge.net/} and DCraw \footnote{https://www.cybercom.net/~dcoffin/dcraw/}. While both tools are open-source and provide image conversion and editing capabilities, UFraw provides a powerful and user-friendly wrapper to the DCraw engine. These tools provided a simple way to fine-tune and convert RAW images for machine learning. 
		
		Research on the image quality - machine learning performance correlation already exists\footnote{https://arxiv.org/pdf/1604.04004.pdf}, and indicates that image deviations that can be compensated for in post-processing have substantial impact on recognition rate. The team therefore decided to proceed to test image optimization via UFraw and DCraw.
		\subsection{Metric Calculation}
		In order to judge the efficacy of image optimization, it was first necessary to establish a metric. Focusing on the Inception-V3 image recognition model, trained on ImageNet\footnote{http://www.image-net.org/} and implemented on Google's Tensorflow CNN framework\footnote{https://www.google.com/url?sa=t\&rct=j\&q=\&esrc=s\&source=web\&cd=1\&ved=0ahUKEwiLjZT008LXAhXoqlQKHXgJB0YQFggoMAA\&url=https\%3A\%2F\%2Fwww.tensorflow.org\%2F\&usg=AOvVaw0TGZBeXHx2CVPI2FiDZclR}, the team decided on a simple binary calculation scheme.
		
		Each image set would focus on a specific keyword, with images of the keyword's subject as well as a distribution of images of related and unrelated entities. The metric would be equal to:
		
		\begin{equation*}
			\text{metric}=\frac{\text{Number of correct top-5 positive identifications}}{\text{number of images}} + \frac{\text{Number of correct negative identifications}}{\text{number of images}}
		\end{equation*}
		
		This calculation, as well as an automated image loading and output parsing script, are present in a set of Python scripts. These are available at a Github repository\footnote{https://github.com/Arcturus314/tensorflow\_loader} as well as in Appendix C.
	\section{Image Set Acquisition}
		\subsection{Image Set 1: Cats}
		The initial keyword chosen was 'cat'. This was due to the large proportion of cats and related cat breeds in the ImageNet database (CITATION REQUIRED), as well as similar quadripedal animals. These characteristics of the ImageNet database would provide a large amount of granularity in the Inception-V3 output. This granularity would allow for a greater level of fine-tuning in ISP pipeline optimization.
		
		There is an animal shelter near Harvey Mudd College, Priceless Pet Rescue\cite{http://pricelesspetrescue.org/}. The initial image set was composed of 179 RAW images of cats and dogs under CFL-lit conditions. There were (X) dogs and (X) cats in this image set. Images were taken in simultaneous RAW-JPG mode with a Canon (ADD MODEL), as (ADD IMAGE DETAILS) pixel resolution. (ADD INFO ON CAMERA SETTINGS)
		
		ADD EXAMPLE IMAGES
		\subsection{Image Set 2: Cats}
		Realizing that a set of images taken under optimal, well-lit, conditions are likely not representative of images in potential applications of the team's ISP, the team elected to take another set of images at the Priceless Pet Rescue. There were (X) number of cats and (X) number of dogs in this image set for a total of (X) images. Pictures were again taken in simultaneous RAW-JPG mode on a Canon (X), except with unoptimal ISO, shutter speed, camera shake, and other parameters.
		
		ADD EXAMPLE IMAGES
		\subsection{Image Set 3: Squash}
		The third and final image set was taken with an unrelated subject --squash-- to help ensure that identification rates were not directly correlated with the cat subject. This image set was taken in simultaneous RAW-JPG mode on a Canon Eos 80D, under outdoor nighttime conditions. Negative samples were of street signs, food products, and other similar objects.
		
		ADD EXAMPLE IMAGES
	\section{Experimental Design}
		The team had two goals in pipeline testing: to ensure that it was possible for the open-source DCraw engine to provide acceptable recognition rates (90\%+), and to minimize the number and complexity of image processing steps required to achieve these recognition rates.
		\subsection{Overall Parameter Testing}
		The first step in this investigation was to examine whether the DCraw engine can match the Inception-V3 binary recognition rate of the integrated camera ISP, and, if so, the parameter distribution required to do so.
		
		For each image set, 46 (CHECK) converted sets were generated by UFraw, each with a different set of parameter distributions. The settings of each UFraw parameter are shown in Figure \ref{ufraw_settings}, with all parameters not described by the given settings left at their defaults, such that an image set exists for every parameter value. A parameter description can be seen on the UFraw man page\footnote{https://www.freebsd.org/cgi/man.cgi?query=ufraw\&manpath=FreeBSD+Ports+7.0-RELEASE}.
		
		\begin{figure}
			\begin{center}
				\label{ufraw_settings}
				\caption{UFraw parameter test settings}
				\begin{tabular}{cccccc}
					temperature & green & gamma & exposure & saturation & black-point \\
					2700 & 0.2 & 0.1 & -3 & 0 & 0 \\
					3000 & 0.6 & 0.3 & -2 & 1 & 0.2 \\
					3300 & 1 & 0.5 & -1 & 2 & 0.4 \\
					3600 & 1.4 & 0.7 & 0 & 3 & 0.6 \\
					3900 & 1.8 & 0.9 & 1 & 4 & 0.8 \\
					4200 & 2.2 && 2 & 5 & 1 \\
					4500 &&& 3 & 6 & \\
					4800 &&&& 7 & \\
					5100 &&&& 8 & \\
					5400 &&&&& \\
					5700 &&&&& \\
					6000 &&&&& \\
					6300 &&&&& \\
				\end{tabular}	
			\end{center}
		\end{figure}
	
		A full parameter command list is available in Section \ref{ufrawcommands}.
		
		After image set generation was completed, the Tensorflow test pipeline scripts in Section \ref{pipelinecode} were used to generate output metrics for each set of UFraw parameters.
		
		\subsection{Pipeline Step Testing}
		After image generation via UFraw, the team's next goal was to minimize the pipeline steps required to achieve adequate recognition rates. These tests were run without the UFraw frontend, as the raw DCraw engine provides much more control over applied ISP steps.
		
		
		The team first eliminated all DCraw steps not found in the default configuration. As shown in the previous parameter testing, the default DCraw conversion pipeline is capable of producing images with adequate recognition rates. 
		
		The condensed DCraw pipeline is shown in Figure (FIGURE NAME).
		
		\begin{figure}
			\begin{center}
				\label{condenseddcraw}
				\caption{The DCRaw option set to test}
				\begin{tabular}{ll}
					Conversion Step & Algorithm Options \\
				\end{tabular}
					
			\end{center}
		\end{figure}
		
	\section{Results}
	\section{Data Analysis}
	\section{Conclusion}

\chapter{Spring Plans}

\chapter{Appendix}

\section{Condensed DCraw Codebase}

\section{Verilog ISP Implementation}

\section{Tensorflow Testing Pipeline} \label{pipelinecode}

\section{UFraw Test Commands} \label{ufrawcommands}




\end{document}